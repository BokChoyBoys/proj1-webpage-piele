<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Pierre Le</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://bokchoyboys.github.io/proj1-webpage-piele/proj3-1/index.html">https://bokchoyboys.github.io/proj1-webpage-piele/proj3-1/index.html</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>

<p>All of the text in your write-up should be <em>in your own words</em>. If you need to add additional HTML features to this document, you can search the <a href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions. To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names appropriately.</p>
<o>The website writeup is intended to be a self-contained walkthrough of the assignment: we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images as well as written explanations about what you did to complete each part of the assignment. Try to be as clear and organized as possible when writing about your own output files or extensions to the assignment. We want to understand what you've achieved and how you've done it!</p> 
<p>If you are well-versed in web development, feel free to ditch this template and make a better looking page.</p>


<p>Here are a few problems students have encountered in the past. Test your website on the instructional machines early!</p>
<ul>
<li>Your main report page should be called index.html.</li>
<li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
<li>Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre></li>
<li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre></li>
<li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
<li>And again, test your website on the instructional machines early!</li>
</ul>


<p>Here is an example of how to include a simple formula:</p>
<p align="middle"><pre align="middle">a^2 + b^2 = c^2</pre></p>
<p>or, alternatively, you can include an SVG image of a LaTex formula.</p>

<div>

<h2 align="middle">Overview</h2>
<p>
    In this project, we implement rendering with path tracing. In the first part, we implement rays and primitatives object to test for intersection. In the next part, we use BVH data structure to optimize checking for intersections. In the third part, we implement direct illumination with zero and one bounce radiance. For the fourth part, we implement global illumination which is bounces above one. The final part we optimize sampling by adaptitively sampling different pixels.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
    We are intially given image coordinates. We want to transform it into camera space coordinates to create a ray. We let $$u = \tan(0.5 * hFov)$$ and $$v = \tan(0.5 * vFov)$$ where $hFov$ and $vFov$ are in radians. Our transformation matrix from image to camera space is $$\begin{bmatrix} 2u & 0 & -u \\ 0 & 2v & -v \\ 0 & 0 & 1\end{bmatrix}$$. We set the $Z = -1$ and then we have to transform the camera space coordinates into world space. Using the location of the camera as our ray origin and the normalized world space coordinates as our ray direction, we create a new ray. To get the image coordinates, we sample randomly within a pixel. We use it to get a ray which we use to estimate the integeral of the radiance of the pixel. For the intersection of a triangle, we use the closed form solution from lecture to calculate $t$ which is a time of intersection and the intersection points of the plane. We check if $t$ is in our min_t and max_t of the ray and use barycentric coordinates to see if the intersection point on the plane is within our triangle. If it does intersect, we update the intersection data structure. Similarly with the sphere intersection, we use the closed form solution from lecture to get intersection points. For a sphere, there can be two intersections so we find the smaller one. We check if the intersection was on the sphere if the distance from the intersection to the origin on the sphere is the same as its radius. If there's an intersection, then we update the intersection structure.

</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
  <p>
    For the triangle intersection algorithm, we use the MÃ¶ller Trumbore algorithm which gets us a possible intersection and the point of intersection. We check if our $t$ is within the bounds of min_t and max_t. We observe that the triangle is in a plane so a intersection could be on the plane but outside the triangle. To check if the intersection point is in the triangle, we use barycentric coordinates to test. Our intersection points can be used as $\alpha, \beta, \gamma$ and see if they are all between 0 and 1. If there is an intersection, we set the ray max_t to the minimum of max_t and $t$. Our intersection normal is a weighted sum of the normals of the triangle vertices using $\alpha, \beta, \gamma$.
  </p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task11.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
      <td>
        <img src="images/task12.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    First, we loop over the iterator to find the size from start to end and the bounding box of all the primatives in the iterator from start to end. We create a new BVH node. If size is less or equal to max_leaf_size, then the node we created is a leaf node and we set the node->start and node->end. Else, we find a split of the bounding box and recurse on the left and right boxes and set those to their respective left and right pointer. We do this by sorting and updating the start and end iterator pointers. My heuristic was to sort by the centroid of all axis, split by the median and choose which axis has the smallest surface area.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task22.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
      <td>
        <img src="images/task23.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task24.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
      <td>
        <img src="images/task25.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    On my machine, using no BVH, I got around similar times for cow.dae as the instructional machines without BVH (~40 s). For maxplanck.dae and CBlucy.dae, it took a significant amount of time (> 2 mins) so I stopped early. Using BVH, I was able to get all of them to less than 2 seconds. cow.dae had less primatives than the other ones so rendering was possible in a decent amount of time. The other dae had too many primative which slowed down rendering a lot when without a BVH.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
    For uniform hemisphere sampling, we sample based on the number of lights and ns_aa. We sample a direction $w_i$ from the hemisphere and convert it to world space. Using $w_i$ as our direction and the hit point of the ray as our origin, we create a new ray. We check if this new ray interesects with anything and update our $L_{out}$ if it does. We add to $L_{out}$ the BSDF times the emission of the interesection times the cosine of angle of the new ray and normal. After all the samples, we divide by the probability of a sample and the number of samples. Our radiance from one bounce is added with the zero bounce. For importance sampling, we loop through all the lights in the scene. If the light is a point light, we sample once the emitted radiance of the light and get the $w_i$, pdf, and distToLight. We create a new ray with our hitpoint and $w_i$. We check if the new ray doesn't intersect anything up to the distToLight. If it doesn't intersect, we add to $L_{out}$ the BSDF times the emission times the cosine theta divided by the pdf similar to hemisphere. If the light is not a point light, we sample ns_aa times and doing the same thing as the point light part. We return $L_{out}$ and add the zero bounce.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/task31.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/task32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/task33.png" align="middle" width="400px"/>
        <figcaption>dragon.dae (Only point lights ðŸ˜”)</figcaption>
      </td>
      <td>
        <img src="images/task34.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/task35.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/task36.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task37.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task38.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task39.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task310.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    The noise level decreases as there are more light rays. 1 and 4 rays have fairly noticable noise while 16 and 64 have little noise and less noticable. The noise levels are distinct and there is better quality as more rays are added from 1 to 64. More rays cause longer rendering times so there may be diminishing returns in how they look and noise levels if there are more rays to be added.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    Uniform sampling has fairly noticeable noise whereas lighting sampling has very little if not no noise. Uniform hemisphere sampling also doesn't work with only point lights so rendering the dragon is not possible. Uniform hemisphere sampling's noise level is related to the sample rate since it needs enough samples to converge. This would vary in computation time. Light samping works with point lights so we are able to render the dragon. Uniform hemisphere sampling samples from uniform directions in a hemisphere while light sampling samples lights. Light sampling can also have a lot of noise depending on the sample rate but is able to have less noise overall compared to hemisphere at similar sampling rates.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    We have a recursive function that stops when we reach our max ray depth or by russian roulette. We calculate the one bounce. If we flip a biased coin of $p = 0.3$ of heads, and it is heads then we stop and return the one bounce else we continue. We call sample_f to get the BSDF and get the $w_i$ and pdf. We change the $w_i$ to world space and create a new ray with $w_i$ and hit point. We check if the new ray intersects with anything. If it does and the cosine theta is above 0, then we add to $L_{out}$ the next bounce of the intersection and new ray times the BSDF times the cosine theta divided by the pdf divided by the continutation probability. We return $L_{out}$ and add it to zero bounce.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task410.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/task49.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task416.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/task417.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Direct illumination is the zero and one bounce radiance. This is turn makes it more bright than above one bounce since zero bounce is essentially the light source and one bounce is the radiance of the light source. Above one bounce basically has a radiance of the sum of radiance. Intuitively, this makes it less bright because of conservation of energy since some light might be absorbed. For direct illumination, the shadows are more darker since light couldn't reach there. In the picture for above one bounce, mostly everything is somewhat lit up besides the light source. If we combine the pictures, then it is essentially the same as the global illumination.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task411.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/task412.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task413.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/task414.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task415.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    At 0 max ray depth, this is essentially zero bounce radiance since there is no bounces. At 1 max ray depth, this is one bounce radiance. There is a lot darker shadows and a lot of the scene is not lit up. For 2 max ray depth, the shadows are more lit and we can see the ceiling. The ceiling edges are a little darker than the 3 max ray depth. At 3 max ray depth, the shadows and dark spots are noticeably brighter and the ceiling edges are more lit. At 100 max ray depth, it is hard to tell if some of the darker spots got somewhat brighter. This is because the number of bounces with russian roulette is a geometric random variable so the expected number of bounces is $\frac{1}{p}$. Rays going to depth 100 is highly unlikely and going above 3 bounces is unlikely.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task41.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task42.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task43.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task44.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task45.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task46.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task47.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Increasing the sample rate decreases the noise. At 1, there is a lot of noise. From 2 to 8, there is a overall decrease in noise from the previous. At 16, there is a little noise but it is still noticeable. At 64, noise is still there but a lot less noticeable than the previous. At 1024, there is essentially little noise if not any.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    Adapative sampling, as the name suggests, adapts the amount of sampling on different parts of the image. Pixels have different convergence rates so it is infeasible to sample a pixel that already converged when we would rather sample a pixel that hasn't converged. We know a pixel has converged if its illuminance is very close to the mean of sampled pixel illuminance so far. At each sample we add the illuminance to $s1$ and the squared illuminance to $s2$. If the amount sampled so far is divisible by the batch size, we calculate $I$ by calculating the sample mean with $s1$ and the sample variance with $s2$ and $s1$ which we use to get the standard deviation. We then check if our $I$ is within the maxTolearance of $\mu$ and updating our numsamples to the amount sampled and breaking.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task51.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/task51_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task52.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task52_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
